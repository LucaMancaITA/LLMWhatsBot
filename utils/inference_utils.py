
# Import modules
import os
import torch
from transformers import (
    BitsAndBytesConfig, LlamaTokenizer, LlamaForCausalLM
)
from peft import PeftModel


HUGGING_FACE_API_KEY = os.environ.get("HUGGING_FACE_API_KEY")

def load_finetune_peft_llm(base_model_id, finetuned_llm_id):
    """Load the fine-tuned PEFT.

    Args:
        base_model_id (str): base llm model id.
        finetuned_llm_id (str): peft local model id.

    Returns:
        model (torch.nn.Module): fine-tuned peft model.
    """
    # Bits and bytes ocnfiguration
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )

    # Load tokenizer and base model
    tokenizer = LlamaTokenizer.from_pretrained(
        base_model_id,
        token=HUGGING_FACE_API_KEY)
    model = LlamaForCausalLM.from_pretrained(
        base_model_id,
        quantization_config=bnb_config,
        device_map="auto",
        token=HUGGING_FACE_API_KEY
    )

    # Load fine-tuned peft model
    peft_model = PeftModel.from_pretrained(model, "./" + finetuned_llm_id)

    return peft_model, tokenizer


def answer(input_text, model, tokenizer):
    """Generate an answer given a query.

    Args:
        input_text (str): input query.
        model (torch.nn.Module): LlamaForCasualLm.
        tokenizer (torch.nn.Module): LlamaTokenizer.

    Returns:
        str: outpout text generated by the model.
    """
    # Encode the input text
    input_ids = tokenizer.encode(
        input_text, return_tensors="pt")

    # Run model inference
    generated_ids = model.generate(
        input_ids=input_ids.to('cuda'),
        max_new_tokens=20,
        min_new_tokens=10,
        pad_token_id=tokenizer.eos_token_id)

    #Â Decode the output text
    generated_text = tokenizer.decode(
        generated_ids[0], skip_special_tokens=True)

    return generated_text


def chat_run(user1, user2, model, tokenizer):
    """Run a chat simulation. Type 'stop' to break.

    Args:
        user1 (str): first user.
        user2 (str): second user.
        model (torch.nn.Module): LlamaForCasualLm.
        tokenizer (torch.nn.Module): LlamaTokenizer.
    """
    for i in range(1, 100):
        query = input()
        if query == "stop":
          break
        if i==1:
            query = f"{user1}: {query}| {user2}:"
        if i>1:
            query = response + f"| {user1}: {query}| {user2}:"
        response = "| ".join(answer(query, model, tokenizer).split("| ")[:i*2])
        print(response)
